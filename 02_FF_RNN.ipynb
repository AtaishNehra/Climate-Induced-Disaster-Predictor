{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fef77-0f64-4b7d-b7d5-dda3db7f3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "\n",
    "from typing import Tuple\n",
    "import torch, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b301a8-f5ef-457e-a9ab-0212190bfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward-Forward Class Objects\n",
    "\n",
    "class FFAdamParams(object):\n",
    "    '''\n",
    "    Profile Settings for Adam Optimizer that is used to create an instance of the Adam Optimizer for each\n",
    "    linear layer for the forward-forward algorithm\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.001,\n",
    "                 betas: Tuple[float, float] = (0.9, 0.999),\n",
    "                 eps: float = 1e-08,\n",
    "                 weight_decay: float = 0,\n",
    "                 amsgrad: bool = False):\n",
    "        '''\n",
    "        Parameter settings that is necessary for the Adam Optimizer to create a new instance for each layer\n",
    "        to perform the extended version of stochastic gradient descent\n",
    "\n",
    "        :param lr: Learning Rate (default: 0.001)\n",
    "        :param betas: Beta value (default: (0.9, 0.999))\n",
    "        :param eps: Epsilon (default: 1e-08)\n",
    "        :param weight_decay: Weight Decay (default: 0)\n",
    "        :param amsgrad: Amsgrad (default: False)\n",
    "        '''\n",
    "\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "\n",
    "class FFSGDParams(object):\n",
    "    '''\n",
    "    Profile Settings for SGD Optimizer that is used to create an instance of the Stochastic Gradient Descent\n",
    "    Optimizer for each linear layer for the forward-forward algorithm\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 lr: float = 0.001,\n",
    "                 momentum: float = 0,\n",
    "                 weight_decay: float = 0,\n",
    "                 dampening: float = 0,\n",
    "                 nesterov: bool = False):\n",
    "        '''\n",
    "        Parameter settings that is necessary for the Stochastic Gradient Descent to create a new instance\n",
    "        for each layer to perform stochastic gradient descent\n",
    "        \n",
    "        :param lr: Learning Rate (default: 0.001)\n",
    "        :param momentum: Momentum value (default: 0)\n",
    "        :param weight_decay: Weight Decay (default: 0)\n",
    "        :param dampening: Dampening Value (default: 0)\n",
    "        :param nesterov: Nesterov value (default: False)\n",
    "        '''\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dampening = dampening\n",
    "        self.nesterov = nesterov\n",
    "\n",
    "\n",
    "class FFLinear(torch.nn.Linear):\n",
    "    '''\n",
    "    Geoffrey Hinton's Forward-Forward Linear Layer Algorithm. This layer greedily learns the goodness function\n",
    "    by maximizing the positive signal and minimizing the negative signal through a dual logarithmic function.\n",
    "    Since lengths are normalized away before training the goodness function, signals passed to proceeding layers\n",
    "    do not rely on the goodness function being trained.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 optimizer_params: (FFAdamParams, FFSGDParams, None) = None,\n",
    "                 num_epochs: int = 100,\n",
    "                 thresh: float = 2.0,\n",
    "                 active_func=torch.nn.ReLU(),\n",
    "                 bias: bool = True,\n",
    "                 device=None,\n",
    "                 dtype=None):\n",
    "        \"\"\"\n",
    "        Initialize FF Linear layer by providing tuning parameters, in and out features, number of training epochs,\n",
    "        and settings necessary for a normal linear layer\n",
    "\n",
    "        @param in_features: Total number of in features to train Linear Layer (dtype: int)\n",
    "        @param out_features: Total number of hidden out units for linear layer (dtype: int)\n",
    "        @param optimizer_params: Optimizer parameters to create optimizer (dtype: [FFAdamParams, FFSGDParams, None], default: None)\n",
    "        @param num_epochs: Total number of epochs for training (dtype: int, default: 100)\n",
    "        @param thresh: FF layer threshold for layer goodness (dtype: float, default: 2.0)\n",
    "        @param active_func: Layer Activation Function (dtype: callable, default: torch.nn.ReLU)\n",
    "        @param bias: Add or refrain from using bias weight (dtype: bool, default: True)\n",
    "        @param device: Pytorch device type whether cpu or gpu (default: None)\n",
    "        @param dtype: Data type of data (default: None)\n",
    "        \"\"\"\n",
    "\n",
    "        # Set device to cpu if is None\n",
    "        if device is None:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "        # Make torch.nn.Linear as parent class to FFLinear class\n",
    "        super(FFLinear, self).__init__(in_features, out_features, bias, device, dtype)\n",
    "\n",
    "        # initialize necessary parameters for layer\n",
    "        self.active_func = active_func\n",
    "        self.num_epochs = num_epochs\n",
    "        self.thresh = thresh\n",
    "        self.thread = None\n",
    "\n",
    "        if isinstance(optimizer_params, FFAdamParams):\n",
    "            # Create Adam optimizer if adam parameters are provided\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                              lr=optimizer_params.lr,\n",
    "                                              betas=optimizer_params.betas,\n",
    "                                              eps=optimizer_params.eps,\n",
    "                                              weight_decay=optimizer_params.weight_decay,\n",
    "                                              amsgrad=optimizer_params.amsgrad)\n",
    "        elif isinstance(optimizer_params, FFSGDParams):\n",
    "            # Create SGD optimizer if adam parameters are provided\n",
    "            self.optimizer = torch.optim.SGD(self.parameters(),\n",
    "                                             lr=optimizer_params.lr,\n",
    "                                             momentum=optimizer_params.momentum,\n",
    "                                             weight_decay=optimizer_params.weight_decay,\n",
    "                                             dampening=optimizer_params.dampening,\n",
    "                                             nesterov=optimizer_params.nesterov)\n",
    "        else:\n",
    "            # Set optimizer as None for online layer learning\n",
    "            raise ValueError('Fast Learning isn''t programmed yet')\n",
    "\n",
    "    def forward(self, signal, norm=True):\n",
    "        '''\n",
    "        Apply layer neuron activities on signal for hidden activities\n",
    "\n",
    "        @param signal: Input signal that is either positive or negative\n",
    "        @param norm: Perform normalization to signal before Neuron Activities (dtype: bool, Default: True)\n",
    "        @return: Hidden neuron activities\n",
    "        '''\n",
    "\n",
    "        # Normalize signal if norm parameters is True (Needed so that weights don't explode)\n",
    "        if norm:\n",
    "            signal /= (signal.norm(2, 1, keepdim=True) + 1e-4)\n",
    "\n",
    "        if self.bias:\n",
    "            # Return hidden neuron activities with bias calculation if bias is specified\n",
    "            return self.active_func(torch.matmul(signal, self.weight.T) + self.bias.unsqueeze(0))\n",
    "        else:\n",
    "            # Return hidden neuron activities without bias calculation if bias isn't specified\n",
    "            return self.active_func(torch.matmul(signal, self.weight.T))\n",
    "\n",
    "    def train(self, d_pos, d_neg):\n",
    "        '''\n",
    "        Trains the goodness function concurrently with neuron activities passed to next layer\n",
    "\n",
    "        @param d_pos: Positive Signal necessary for training\n",
    "        @param d_neg: Negative Signal necessary for training\n",
    "        @return: neuron activities\n",
    "        '''\n",
    "\n",
    "        # Normalize each signal to remove the lengths before training\n",
    "        d_pos /= (d_pos.norm(2, 1, keepdim=True) + 1e-4)\n",
    "        d_neg /= (d_neg.norm(2, 1, keepdim=True) + 1e-4)\n",
    "\n",
    "        # Initiate concurrent thread to train goodness function\n",
    "        self.thread = Thread(target=self._learn_goodness_func, args=(d_pos, d_neg))\n",
    "        self.thread.start()\n",
    "\n",
    "        # Pass neuron activities to next layer\n",
    "        return self.forward(d_pos, norm=False).detach_(), self.forward(d_neg, norm=False).detach_()\n",
    "\n",
    "    def _learn_goodness_func(self, d_pos, d_neg):\n",
    "        '''\n",
    "        Trains the goodness function for the layer without backward propagation\n",
    "        \n",
    "        @param d_pos: Positive data signal\n",
    "        @param d_neg: Negative data signal\n",
    "        @return: None\n",
    "        '''\n",
    "\n",
    "        # iterate by epoch to train goodness function\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Find the neuron activities for both positive and negative signals\n",
    "            g_pos, g_neg = self.forward(d_pos, norm=False), self.forward(d_neg, norm=False)\n",
    "            # Calculate the sum of squared activities for both signals\n",
    "            g_pos, g_neg = g_pos.pow(2).mean(dim=-1), g_neg.pow(2).mean(dim=-1)\n",
    "            # Calculate the loss by maximizing positive goodness and minimizing negative goodness\n",
    "            loss = (torch.log(1 + torch.exp(self.thresh - g_pos)) + torch.log(\n",
    "                1 + torch.exp(g_neg - self.thresh))).mean() / 2\n",
    "\n",
    "            # Zero optimizer gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            # Find goodness function gradient\n",
    "            loss.backward()\n",
    "            # Adjust weights with optimizer\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Remove all gradients\n",
    "        for param in self.parameters():\n",
    "            param.grad = None\n",
    "\n",
    "        # Free cuda resources\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Declare FFLayers as all possible layers for the Forward-Forward Algorithm\n",
    "FFLayers = (FFLinear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
