{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fef77-0f64-4b7d-b7d5-dda3db7f3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "\n",
    "from typing import Tuple\n",
    "import torch, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b301a8-f5ef-457e-a9ab-0212190bfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward-Forward Class Objects\n",
    "\n",
    "class FFAdamParams(object):\n",
    "    '''\n",
    "    Profile Settings for Adam Optimizer that is used to create an instance of the Adam Optimizer for each\n",
    "    linear layer for the forward-forward algorithm\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.001,\n",
    "                 betas: Tuple[float, float] = (0.9, 0.999),\n",
    "                 eps: float = 1e-08,\n",
    "                 weight_decay: float = 0,\n",
    "                 amsgrad: bool = False):\n",
    "        '''\n",
    "        Parameter settings that is necessary for the Adam Optimizer to create a new instance for each layer\n",
    "        to perform the extended version of stochastic gradient descent\n",
    "\n",
    "        :param lr: Learning Rate (default: 0.001)\n",
    "        :param betas: Beta value (default: (0.9, 0.999))\n",
    "        :param eps: Epsilon (default: 1e-08)\n",
    "        :param weight_decay: Weight Decay (default: 0)\n",
    "        :param amsgrad: Amsgrad (default: False)\n",
    "        '''\n",
    "\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "\n",
    "class FFSGDParams(object):\n",
    "    '''\n",
    "    Profile Settings for SGD Optimizer that is used to create an instance of the Stochastic Gradient Descent\n",
    "    Optimizer for each linear layer for the forward-forward algorithm\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                 lr: float = 0.001,\n",
    "                 momentum: float = 0,\n",
    "                 weight_decay: float = 0,\n",
    "                 dampening: float = 0,\n",
    "                 nesterov: bool = False):\n",
    "        '''\n",
    "        Parameter settings that is necessary for the Stochastic Gradient Descent to create a new instance\n",
    "        for each layer to perform stochastic gradient descent\n",
    "        \n",
    "        :param lr: Learning Rate (default: 0.001)\n",
    "        :param momentum: Momentum value (default: 0)\n",
    "        :param weight_decay: Weight Decay (default: 0)\n",
    "        :param dampening: Dampening Value (default: 0)\n",
    "        :param nesterov: Nesterov value (default: False)\n",
    "        '''\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.dampening = dampening\n",
    "        self.nesterov = nesterov\n",
    "\n",
    "\n",
    "class FFLinear(torch.nn.Linear):\n",
    "    '''\n",
    "    Geoffrey Hinton's Forward-Forward Linear Layer Algorithm. This layer greedily learns the goodness function\n",
    "    by maximizing the positive signal and minimizing the negative signal through a dual logarithmic function.\n",
    "    Since lengths are normalized away before training the goodness function, signals passed to proceeding layers\n",
    "    do not rely on the goodness function being trained.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 optimizer_params: (FFAdamParams, FFSGDParams, None) = None,\n",
    "                 num_epochs: int = 100,\n",
    "                 thresh: float = 2.0,\n",
    "                 norm_alpha: float = 0.1,\n",
    "                 active_func=torch.nn.ReLU(),\n",
    "                 bias: bool = True,\n",
    "                 device=None,\n",
    "                 dtype=None):\n",
    "        \"\"\"\n",
    "        Initialize FF Linear layer by providing tuning parameters, in and out features, number of training epochs,\n",
    "        and settings necessary for a normal linear layer\n",
    "\n",
    "        @param in_features: Total number of in features to train Linear Layer (dtype: int)\n",
    "        @param out_features: Total number of hidden out units for linear layer (dtype: int)\n",
    "        @param optimizer_params: Optimizer parameters to create optimizer (dtype: [FFAdamParams, FFSGDParams, None], default: None)\n",
    "        @param num_epochs: Total number of epochs for training (dtype: int, default: 100)\n",
    "        @param thresh: FF layer threshold for layer goodness (dtype: float, default: 2.0)\n",
    "        @param active_func: Layer Activation Function (dtype: callable, default: torch.nn.ReLU)\n",
    "        @param bias: Add or refrain from using bias weight (dtype: bool, default: True)\n",
    "        @param device: Pytorch device type whether cpu or gpu (default: None)\n",
    "        @param dtype: Data type of data (default: None)\n",
    "        \"\"\"\n",
    "\n",
    "        # Set device to cpu if is None\n",
    "        if device is None:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "        # Make torch.nn.Linear as parent class to FFLinear class\n",
    "        super(FFLinear, self).__init__(in_features, out_features, bias, device, dtype)\n",
    "\n",
    "        # initialize necessary parameters for layer\n",
    "        self.active_func = active_func\n",
    "        self.num_epochs = num_epochs\n",
    "        self.thresh = thresh\n",
    "        self.threads = list()\n",
    "        self.norm_pos = 1e-4\n",
    "        self.norm_neg = 1e-4\n",
    "        self.norm_alpha = norm_alpha\n",
    "\n",
    "        if isinstance(optimizer_params, FFAdamParams):\n",
    "            # Create Adam optimizer if adam parameters are provided\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                              lr=optimizer_params.lr,\n",
    "                                              betas=optimizer_params.betas,\n",
    "                                              eps=optimizer_params.eps,\n",
    "                                              weight_decay=optimizer_params.weight_decay,\n",
    "                                              amsgrad=optimizer_params.amsgrad)\n",
    "        elif isinstance(optimizer_params, FFSGDParams):\n",
    "            # Create SGD optimizer if adam parameters are provided\n",
    "            self.optimizer = torch.optim.SGD(self.parameters(),\n",
    "                                             lr=optimizer_params.lr,\n",
    "                                             momentum=optimizer_params.momentum,\n",
    "                                             weight_decay=optimizer_params.weight_decay,\n",
    "                                             dampening=optimizer_params.dampening,\n",
    "                                             nesterov=optimizer_params.nesterov)\n",
    "        else:\n",
    "            # Set optimizer as None for online layer learning\n",
    "            self.optimizer = None\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, signal, norm=True):\n",
    "        '''\n",
    "        Apply layer neuron activities on signal for hidden activities\n",
    "\n",
    "        @param signal: Input signal that is either positive or negative\n",
    "        @param norm: Perform normalization to signal before Neuron Activities (dtype: bool, Default: True)\n",
    "        @return: Hidden neuron activities\n",
    "        '''\n",
    "\n",
    "        # Normalize signal if norm parameters is True (Needed so that weights don't explode)\n",
    "        if norm:\n",
    "            # Update normalization using average of positive and negative norms\n",
    "            signal /= (((self.norm_pos + self.norm_neg) / 2) + 1e-4)\n",
    "\n",
    "        if self.optimizer and torch.all(self.bias):\n",
    "            # Return hidden neuron activities with bias calculation if bias is specified\n",
    "            return self.active_func(torch.matmul(signal, self.weight.T) + self.bias.unsqueeze(0))\n",
    "        else:\n",
    "            # Return hidden neuron activities without bias calculation if bias isn't specified\n",
    "            return self.active_func(torch.matmul(signal, self.weight.T))\n",
    "\n",
    "    def train(self, d_pos, d_neg):\n",
    "        '''\n",
    "        Trains the goodness function concurrently with neuron activities passed to next layer\n",
    "\n",
    "        @param d_pos: Positive Signal necessary for training\n",
    "        @param d_neg: Negative Signal necessary for training\n",
    "        @return: neuron activities\n",
    "        '''\n",
    "\n",
    "        # ensure that multiple threads aren't created if optimizer is specified. This messes up with the optimizer\n",
    "        if self.threads and self.optimizer:\n",
    "            for thread in self.threads:\n",
    "                thread.join()\n",
    "\n",
    "        # Find L2 norm for each positive and negative signals\n",
    "        pos_denom = d_pos.norm(2, 1, keepdim=True)\n",
    "        neg_denom = d_neg.norm(2, 1, keepdim=True)\n",
    "\n",
    "        # Estimate global L2 normalization with exponential moving average\n",
    "        self.norm_pos = self.norm_alpha * pos_denom.mean() + (\n",
    "                1 - self.norm_alpha) * self.norm_pos\n",
    "        self.norm_neg = self.norm_alpha * neg_denom.mean() + (\n",
    "                1 - self.norm_alpha) * self.norm_neg\n",
    "\n",
    "        # Normalize each signal to remove the lengths before training\n",
    "        d_pos /= (pos_denom + 1e-4)\n",
    "        d_neg /= (neg_denom + 1e-4)\n",
    "\n",
    "        # Initiate concurrent thread to train goodness function\n",
    "        thread = Thread(target=self._learn_goodness_func, args=(d_pos, d_neg))\n",
    "        thread.start()\n",
    "        self.threads.append(thread)\n",
    "\n",
    "        # Pass neuron activities to next layer\n",
    "        return self.forward(d_pos, norm=False).detach_(), self.forward(d_neg, norm=False).detach_()\n",
    "\n",
    "    def _learn_goodness_func(self, d_pos, d_neg):\n",
    "        '''\n",
    "        Trains the goodness function for the layer without backward propagation\n",
    "\n",
    "        @param d_pos: Positive data signal\n",
    "        @param d_neg: Negative data signal\n",
    "        @return: None\n",
    "        '''\n",
    "\n",
    "        # iterate by epoch to train goodness function\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Find the neuron activities for both positive and negative signals\n",
    "            y_pos, y_neg = self.forward(d_pos, norm=False), self.forward(d_neg, norm=False)\n",
    "            # Calculate the squared activities for both signals\n",
    "            g_pos, g_neg = y_pos.pow(2), y_neg.pow(2)\n",
    "\n",
    "            if self.optimizer:\n",
    "                # Calculate the loss by maximizing positive goodness and minimizing negative goodness\n",
    "                loss = (torch.log(1 + torch.exp(self.thresh - g_pos.mean(dim=-1))) + torch.log(\n",
    "                    1 + torch.exp(g_neg.mean(dim=-1) - self.thresh))).mean() / 2\n",
    "                # Zero optimizer gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                # Find goodness function gradient\n",
    "                loss.backward()\n",
    "                # Adjust weights with optimizer\n",
    "                self.optimizer.step()\n",
    "            else:\n",
    "                # Calculate epsilon according to Learning Fast - FF Algorithm\n",
    "                epsilon = torch.sqrt(self.thresh / g_pos.sum()) - 1\n",
    "\n",
    "                # Calculate Log of Probabilities using squared activities for both positive and negative signals\n",
    "                loss = (\n",
    "                           torch.log(1 + torch.exp(self.thresh - g_pos.mean(dim=-1))) / g_pos.sum() +\n",
    "                           torch.log(1 + torch.exp(g_neg.mean(dim=1) - self.thresh)) / g_neg.sum()\n",
    "                   ).mean()\n",
    "\n",
    "                # Perform derivative of loss function\n",
    "                loss.backward()\n",
    "\n",
    "                # Disable Gradient calculation\n",
    "                with torch.no_grad():\n",
    "                    # Update weight according to Learning Fast & Slow formula\n",
    "                    self.weight -= epsilon * self.weight.grad * (y_pos.T @ d_pos)\n",
    "\n",
    "                # Remove gradient for next weight calculation\n",
    "                self.weight.grad = None\n",
    "\n",
    "        # Free cuda resources\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "class FFRNN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 batch_size: int,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 num_of_layers: int = 3,\n",
    "                 num_epochs: int = 60,\n",
    "                 thresh: float = 2.0,\n",
    "                 active_func=torch.nn.ReLU(),\n",
    "                 optimizer_params: (FFAdamParams, FFSGDParams, None) = None,\n",
    "                 bias: bool = True,\n",
    "                 device=None,\n",
    "                 dtype=None):\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(FFRNN, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.a = torch.nn.Parameter(torch.zeros([batch_size, out_features, out_features],\n",
    "                                                **factory_kwargs).type(torch.float32))\n",
    "        self.h = torch.nn.Parameter(torch.zeros([batch_size, out_features],\n",
    "                                                **factory_kwargs).type(torch.float32))\n",
    "        self.layers = torch.nn.ModuleList([FFLinear(in_features=in_features if i == 0 else out_features,\n",
    "                                                    out_features=out_features,\n",
    "                                                    optimizer_params=optimizer_params,\n",
    "                                                    num_epochs=num_epochs,\n",
    "                                                    thresh=thresh,\n",
    "                                                    active_func=active_func,\n",
    "                                                    bias=bias,\n",
    "                                                    dtype=dtype\n",
    "                                                    ) for i in range(num_of_layers)])\n",
    "\n",
    "    def train(self, d_pos, d_neg):\n",
    "        pass\n",
    "\n",
    "\n",
    "# Declare FFLayers as all possible layers for the Forward-Forward Algorithm\n",
    "FFLayers = (FFLinear, FFRNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
